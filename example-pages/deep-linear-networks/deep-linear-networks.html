<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>deep-linear-networks</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
  </style>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<link rel="stylesheet" href="modest.css">
<style>
pre, code, pre code {
  max-height: 400px;
}
</style>
<h2 id="deep-linear-networks">Deep Linear Networks</h2>
<h4 id="authors-colin-crowley-and-jose-israel-rodriguez-jacob-weiker-and-jacob-zoromski">Authors: <a href="https://sites.google.com/view/colincrowley/home">Colin Crowley</a>, and <a href="https://www.math.wisc.edu/~jose/">Jose Israel Rodriguez</a>, Jacob Weiker, and Jacob Zoromski</h4>
<p>One approach to gain a better theoretical understanding of deep neural networks is by characterizing the loss surface and regularizers. In particular, one might want to know how many of the local optimum are global optimum with a regularization.</p>
<h3 id="defining-equations">Defining equations</h3>
<p>We consider <em>deep linear networks</em> which are specified by the weight matrices <span class="math inline">\(\mathbf{W}:=(W_1,\dots,W_{H+1})\)</span> with <span class="math inline">\(H\)</span> denoting the number of hidden layers and <span class="math inline">\(W_i \in \mathbb{R}^{d_{i}\times d_{i-1}}\)</span>.</p>
<p>We use the usual squared Euclidean distance loss function <span class="math display">\[
\mathcal{L}_{\mathbf{X},\mathbf{Y}}(\mathbf{W}):=
\frac{1}{2}\sum_{j=1}^m
|| W_{H+1}\cdots W_1 X_j -Y_j 
|| ^2.
\]</span> and a <em>generalized Tikhonov regularization term</em> <span class="math display">\[
\mathcal{R}_\Lambda(\mathbf{W}):=
\sum_{i=1}^{H+1} ||  \Lambda_i \star W_i||^2  
\]</span> where for <span class="math inline">\(i=1,\dots,H+1\)</span>, <span class="math inline">\(\Lambda_i\)</span> is a generic <span class="math inline">\(d_{i}\times d_{i-1}\)</span> matrix, <span class="math inline">\(X_j,Y_j\)</span> are constant vectors, and <span class="math inline">\(\star\)</span> denotes the Hadamard (coordinate wise) product. Given data <span class="math inline">\((\mathbf{X},\mathbf{Y})\)</span> one wishes to find a global minima of the loss function <span class="math inline">\(\mathcal{L}_{\mathbf{X},\mathbf{Y}}\)</span> and its regularization <span class="math inline">\(\mathcal{L}_{\mathbf{X},\mathbf{Y}}+\mathcal{R}_\Lambda\)</span>.</p>
<p>After complexification, the variety of partial derivatives with respect to the weight indeterminants defines a set of critical points and a complex algebraic variety. The number of critical points and loss function are studied <a href="https://arxiv.org/pdf/1810.07716.pdf">here</a>. We apply our regeneration method to these problems and obtain the computational results on the right side of the table below. The number of critical points shown in the left side of the table were known already, but with our work, we discovered the explicit sparsity pattern structure. This sparsity structure coincides directly with the topology of the network as a zero corresponds to a missing edge in the network.</p>
<p><img src="table.png" class="center"></p>
<h3 id="runing-multiregeneration.py">Runing <code>multiregeneration.py</code></h3>
<p>The files to run the cases <span class="math inline">\((d_0,d_1,d_2) = (2,2,2)\)</span> and <span class="math inline">\((d_0,d_1,d_2,d_3) = (2,2,2,2)\)</span> are below.</p>
<h4 id="d_0d_1d_2-222"><span class="math inline">\((d_0,d_1,d_2) = (2,2,2)\)</span></h4>
<p><a href="D_2_2_2/inputFile.py">inputFile.py</a></p>
<p><a href="D_2_2_2/bertiniInput_variables">bertiniInput_variables</a></p>
<p><a href="D_2_2_2/bertiniInput_equations">bertiniInput_equations</a></p>
<p><a href="D_2_2_2/bertiniInput_trackingOptions">bertiniInput_trackingOptions</a></p>
<h4 id="d_0d_1d_2-d_3-2222"><span class="math inline">\((d_0,d_1,d_2, d_3) = (2,2,2,2)\)</span></h4>
<p><a href="D_2_2_2_2/inputFile.py">inputFile.py</a></p>
<p><a href="D_2_2_2_2/bertiniInput_variables">bertiniInput_variables</a></p>
<p><a href="D_2_2_2_2/bertiniInput_equations">bertiniInput_equations</a></p>
<p><a href="D_2_2_2_2/bertiniInput_trackingOptions">bertiniInput_trackingOptions</a></p>
</body>
</html>
